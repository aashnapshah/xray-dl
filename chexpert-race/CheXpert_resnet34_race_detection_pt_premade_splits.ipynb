{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import glob\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import wandb\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import random as python_random\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve, precision_recall_curve\n",
    "from sklearn.metrics import auc, accuracy_score, recall_score, precision_score, f1_score, confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "import sys\n",
    "import math\n",
    "\n",
    "from torch.utils.data import random_split, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CheXpert images can be found: https://stanfordaimi.azurewebsites.net/datasets/8cbd9ed4-2eb9-4565-affc-111cf4f7ebe2\n",
    "data_df = pd.read_csv('train_cheXbert.csv')\n",
    "\n",
    "# Demographic labels can be found: https://stanfordaimi.azurewebsites.net/datasets/192ada7c-4d43-466e-b8bb-b81992bb80cf\n",
    "demo_df = pd.DataFrame(pd.read_excel(\"CHEXPERT_DEMO.xlsx\", engine='openpyxl')) #pip install openpyxl\n",
    "\n",
    "# 60-10-30, train-val-test split that we used\n",
    "# These splits can be found in this repository\n",
    "split_df = pd.read_csv('chexpert_split_2021_08_20.csv').set_index('index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.concat([data_df,split_df], axis=1)\n",
    "data_df = data_df[~data_df.split.isna()]\n",
    "\n",
    "path_split =  data_df.Path.str.split(\"/\", expand = True)\n",
    "data_df[\"patient_id\"] = path_split[2]\n",
    "demo_df = demo_df.rename(columns={'PATIENT': 'patient_id'})\n",
    "data_df = data_df.merge(demo_df, on=\"patient_id\")\n",
    "\n",
    "mask = (data_df.PRIMARY_RACE.str.contains(\"Black\", na=False))\n",
    "data_df.loc[mask, \"race\"] = \"BLACK/AFRICAN AMERICAN\"\n",
    "\n",
    "mask = (data_df.PRIMARY_RACE.str.contains(\"White\", na=False))\n",
    "data_df.loc[mask, \"race\"] = \"WHITE\"\n",
    "\n",
    "mask = (data_df.PRIMARY_RACE.str.contains(\"Asian\", na=False))\n",
    "data_df.loc[mask, \"race\"] = \"ASIAN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train       0.599482\n",
       "test        0.300823\n",
       "validate    0.099695\n",
       "Name: split, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.split.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WHITE                     0.779016\n",
       "ASIAN                     0.148130\n",
       "BLACK/AFRICAN AMERICAN    0.072854\n",
       "Name: race, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.race.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "split     race                  \n",
       "train     WHITE                     0.466008\n",
       "test      WHITE                     0.234774\n",
       "train     ASIAN                     0.089452\n",
       "validate  WHITE                     0.078234\n",
       "test      ASIAN                     0.044447\n",
       "train     BLACK/AFRICAN AMERICAN    0.044022\n",
       "test      BLACK/AFRICAN AMERICAN    0.021602\n",
       "validate  ASIAN                     0.014231\n",
       "          BLACK/AFRICAN AMERICAN    0.007230\n",
       "dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df[['split', 'race']].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ASIAN                     100\n",
       "BLACK/AFRICAN AMERICAN    100\n",
       "WHITE                     100\n",
       "Name: race, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = data_df[data_df.split==\"train\"]\n",
    "validation_df = data_df[data_df.split==\"validate\"]\n",
    "test_df = data_df[data_df.split==\"test\"]\n",
    "\n",
    "size = 100 \n",
    "# Perform stratified sampling to get 5000 samples for each group within the \"race\" column\n",
    "train_stratified = train_df.groupby(\"race\", group_keys=False).apply(lambda x: x.sample(min(len(x), size), random_state=42))\n",
    "validation_stratified = validation_df.groupby(\"race\", group_keys=False).apply(lambda x: x.sample(min(len(x), size), random_state=42))\n",
    "test_stratified = test_df.groupby(\"race\", group_keys=False).apply(lambda x: x.sample(min(len(x), size), random_state=42))\n",
    "\n",
    "train_stratified.race.value_counts()\n",
    "validation_stratified.race.value_counts()\n",
    "test_stratified.race.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('train_df.csv')\n",
    "validation_df.to_csv('validation_df.csv')\n",
    "test_df.to_csv('test_df.csv')\n",
    "\n",
    "train_stratified.to_csv('train_sub_df.csv')\n",
    "validation_stratified.to_csv('validation_sub_df.csv')\n",
    "test_stratified.to_csv('test_sub_df.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#False indicates no patient_id shared between groups\n",
    "\n",
    "unique_train_id = train_df.patient_id.unique()\n",
    "unique_validation_id = validation_df.patient_id.unique()\n",
    "unique_test_id = test_df.patient_id.unique()\n",
    "all_id = np.concatenate((unique_train_id, unique_validation_id, unique_test_id), axis=None)\n",
    "\n",
    "def contains_duplicates(X):\n",
    "    return len(np.unique(X)) != len(X)\n",
    "\n",
    "contains_duplicates(all_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet50, resnet34\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.cuda.amp as amp \n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "# torch.manual_seed(2021)\n",
    "# torch.cuda.manual_seed(2021)\n",
    "\n",
    "class CustomResNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CustomResNet, self).__init__()\n",
    "        self.resnet34 = resnet34(pretrained=True)\n",
    "        self.features = nn.Sequential(*list(self.resnet34.children())[:-2])\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc =  nn.Sequential(nn.Linear(512, num_classes), nn.Softmax(dim=1)  # Apply softmax for probability distribution\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Define dataset and dataloaders\n",
    "class DatasetGenerator(Dataset):\n",
    "    def __init__(self, data_frame, root_dir, nnTarget, transform=None):\n",
    "        self.data_frame = pd.read_csv(data_frame)\n",
    "        self.target = nnTarget\n",
    "        self.listImagePaths = list(root_dir + self.data_frame['Path'])\n",
    "        self.listImageLabels = list(self.data_frame[nnTarget])\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        imagePath = self.listImagePaths[index]\n",
    "        imageData = Image.open(imagePath).convert('RGB')\n",
    "\n",
    "        label = self.listImageLabels[index]\n",
    "        # Define a dictionary to map class labels to class indices\n",
    "        class_to_idx = {\n",
    "            'ASIAN': 0,\n",
    "            'WHITE': 1,\n",
    "            'BLACK/AFRICAN AMERICAN': 2\n",
    "        }\n",
    "        imageLabel = class_to_idx[label]\n",
    "        if self.transform != None: imageData = self.transform(imageData)\n",
    "\n",
    "        # if self.target == 'race':\n",
    "        #     possible_labels = self.data_frame[self.target].unique() # List all possible categories\n",
    "            \n",
    "        #     # Convert categorical label to one-hot encoded tensor\n",
    "        #     one_hot_label = torch.zeros(len(possible_labels))\n",
    "        #     label_index = possible_labels.tolist().index(label)\n",
    "\n",
    "        #     one_hot_label[label_index] = 1.0\n",
    "\n",
    "        #     # Convert to torch.FloatTensor\n",
    "        #     imageLabel = torch.FloatTensor(one_hot_label)        \n",
    "        #     if self.transform != None: imageData = self.transform(imageData)\n",
    "        # else:\n",
    "        #     # Convert to torch.FloatTensor\n",
    "        #     imageLabel = label\n",
    "        #     if self.transform != None: imageData = self.transform(imageData)\n",
    "\n",
    "        return imageData, imageLabel\n",
    "\n",
    "HEIGHT, WIDTH = 320, 320\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((320, 320)),\n",
    "    transforms.RandomRotation(15),      # Randomly rotate the image by up to 15 degrees\n",
    "    transforms.RandomResizedCrop(320, scale=(0.9, 1.1)), \n",
    "    transforms.RandomHorizontalFlip(),  # Horizontal Flip\n",
    "    transforms.ToTensor(),  # Convert to Tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize with ImageNet stats\n",
    "])\n",
    "\n",
    "# Define preprocessing transformations\n",
    "validate_transform = transforms.Compose([\n",
    "    transforms.Resize((320, 320)),                   # Resize the input image to 256x256\n",
    "    transforms.CenterCrop(320),               # Crop the center 224x224 portion of the image\n",
    "    transforms.ToTensor(),                    # Convert to a PyTorch tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize with ImageNet stats\n",
    "])\n",
    "\n",
    "train_dataset = DatasetGenerator('train_sub_df.csv', '../', 'race', transform=train_transform) \n",
    "validate_dataset = DatasetGenerator('validation_sub_df.csv',  '../', 'race', transform=validate_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 320, 320])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "momentum_val=0.9\n",
    "decay_val= 0.0\n",
    "train_batch_size = 256 # may need to reduce batch size if OOM error occurs\n",
    "test_batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 2\n"
     ]
    }
   ],
   "source": [
    "train_epoch = math.ceil(len(train_dataset) / train_batch_size)\n",
    "val_epoch = math.ceil(len(validate_dataset) / test_batch_size)\n",
    "print(train_epoch, val_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aas926/miniconda3/envs/xray_env/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/aas926/miniconda3/envs/xray_env/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/home/aas926/miniconda3/envs/xray_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "# Create an instance of the ResNet-34 model\n",
    "model = resnet34(pretrained=True)\n",
    "\n",
    "# Modify the final fully connected layer for your specific task\n",
    "num_classes = 3  # Replace with the number of classes in your task\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "model = model.cuda()\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True, num_workers=32)\n",
    "validate_loader = DataLoader(validate_dataset, batch_size=test_batch_size, shuffle=False, num_workers=32)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=decay_val)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, min_lr=1e-5, verbose=True)\n",
    "criterion = nn.CrossEntropyLoss()  # Use appropriate loss function here\n",
    "\n",
    "record_wb = False\n",
    "if record_wb == True: \n",
    "        wandb.init(\n",
    "                # set the wandb project where this run will be logged\n",
    "                project=\"chexnet-\" + 'race' + \"-pred\",\n",
    "                # track hyperparameters and run metadata\n",
    "                config={\"architecture\": \"ResNet34\",\n",
    "                        \"dataset\": \"CheXpert\",\n",
    "                        \"fine-tuned\": True, \n",
    "                        \"target\": 'race', \n",
    "                        \"data-subset\": False\n",
    "                }\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aas926/miniconda3/envs/xray_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Train Loss: 1.1747, Train Accuracy: 0.3533,           Val Loss: 1.1707, Val Accuracy: 0.3933, Val AUROC: 0.5626, Val AUC: 0.3948\n",
      "model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aas926/miniconda3/envs/xray_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Train Loss: 1.0775, Train Accuracy: 0.4333,           Val Loss: 1.4691, Val Accuracy: 0.3200, Val AUROC: 0.5549, Val AUC: 0.4057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aas926/miniconda3/envs/xray_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Train Loss: 1.1184, Train Accuracy: 0.3867,           Val Loss: 1.3570, Val Accuracy: 0.3567, Val AUROC: 0.5748, Val AUC: 0.3977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aas926/miniconda3/envs/xray_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Train Loss: 1.1066, Train Accuracy: 0.3500,           Val Loss: 1.2274, Val Accuracy: 0.3567, Val AUROC: 0.5703, Val AUC: 0.3972\n",
      "Epoch 00004: reducing learning rate of group 0 to 1.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aas926/miniconda3/envs/xray_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Train Loss: 1.0552, Train Accuracy: 0.4533,           Val Loss: 1.1452, Val Accuracy: 0.3800, Val AUROC: 0.5703, Val AUC: 0.4043\n",
      "model saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aas926/miniconda3/envs/xray_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Train Loss: 1.0020, Train Accuracy: 0.4900,           Val Loss: 1.1702, Val Accuracy: 0.4067, Val AUROC: 0.5840, Val AUC: 0.4221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aas926/miniconda3/envs/xray_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Train Loss: 0.9624, Train Accuracy: 0.5300,           Val Loss: 1.1609, Val Accuracy: 0.4433, Val AUROC: 0.6070, Val AUC: 0.4459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aas926/miniconda3/envs/xray_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Train Loss: 0.9633, Train Accuracy: 0.5367,           Val Loss: 1.2289, Val Accuracy: 0.4233, Val AUROC: 0.5834, Val AUC: 0.4278\n",
      "Epoch 00008: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aas926/miniconda3/envs/xray_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Train Loss: 0.9535, Train Accuracy: 0.5467,           Val Loss: 1.1820, Val Accuracy: 0.4267, Val AUROC: 0.6063, Val AUC: 0.4429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aas926/miniconda3/envs/xray_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Train Loss: 0.9508, Train Accuracy: 0.5367,           Val Loss: 1.1801, Val Accuracy: 0.4333, Val AUROC: 0.6049, Val AUC: 0.4405\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "best_val_loss = float('inf')\n",
    "best_model_weights = None\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    train_total = 0 \n",
    "\n",
    "    val_predictions = []  # Store predicted probabilities for AUROC and PR-AUC\n",
    "    val_labels = []  # S\n",
    "\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.cuda()\n",
    "        labels = labels.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        probs = torch.nn.functional.softmax(outputs, dim=1)\n",
    "        _, predicted_classes = torch.max(probs, 1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += torch.sum(predicted_classes==labels).item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    train_accuracy = train_correct / train_total\n",
    "\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in validate_loader:\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            probs = torch.nn.functional.softmax(outputs, dim=1)\n",
    "            _, predicted_classes = torch.max(probs, 1)\n",
    "\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += torch.sum(predicted_classes==labels).item()\n",
    "\n",
    "            # Collect predicted probabilities and labels for AUROC and PR-AUC\n",
    "            val_predictions.extend(probs.cpu().numpy())  # Assuming you have 2 classes, using probabilities of class 1\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    val_loss /= len(validate_loader)\n",
    "    val_accuracy = val_correct / val_total\n",
    "\n",
    "    # Calculate AUROC and PR-AUC\n",
    "    label_binarizer = LabelBinarizer().fit(val_labels)\n",
    "    y_onehot_test = label_binarizer.transform(val_labels)\n",
    "\n",
    "    auroc = roc_auc_score(y_onehot_test, val_predictions, multi_class='ovr')\n",
    "    pr_auc = average_precision_score(y_onehot_test, val_predictions)\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \\\n",
    "          Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val AUROC: {auroc:.4f}, Val AUC: {pr_auc:.4f}')\n",
    "    #wandb.log({'epoch': epoch, 'Train Loss': train_loss, 'Train Accuracy': train_accuracy, 'Val Loss': val_loss, 'Val Accuracy': val_accuracy, 'Val AUROC': auroc, 'Val AUC': pr_auc})\n",
    "    # Save the model if validation loss improves\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model_weights = model.state_dict()\n",
    "        arc_name = 'CHEXPERT_RACE_RESNET34_'\n",
    "        var_date = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        model_name = \"models/\" + str(arc_name) + \"_\" + var_date + f\"_epoch:{epoch:03d}_val_loss:{val_loss:.2f}.pth.tar\"\n",
    "        torch.save({'epoch': epoch + 1, 'state_dict': model.state_dict(), 'best_loss': val_loss, 'optimizer' : optimizer.state_dict()}, model_name)\n",
    "        print('model saved')\n",
    "\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "print(\"Training finished.\")\n",
    "#wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multilabel_predict_test = model.predict(test_batches, max_queue_size=10, verbose=1, steps=math.ceil(len(test_df)/test_batch_size), workers=16)\n",
    "# result = multilabel_predict_test\n",
    "# #result = model.predict(validate_batches, val_epoch)\n",
    "# labels = np.argmax(result, axis=1)\n",
    "# target_names = ['Asian', 'Black', 'White']\n",
    "\n",
    "# print ('Classwise ROC AUC \\n')\n",
    "# for p in list(set(labels)):\n",
    "#     fpr, tpr, thresholds = roc_curve(test_batches.classes, result[:,p], pos_label = p)\n",
    "#     auroc = round(auc(fpr, tpr), 2)\n",
    "#     print ('Class - {} ROC-AUC- {}'.format(target_names[p], auroc))\n",
    "\n",
    "# print (classification_report(test_batches.classes, labels, target_names=target_names))\n",
    "# class_matrix = confusion_matrix(test_batches.classes, labels)\n",
    "\n",
    "# sns.heatmap(class_matrix, annot=True, fmt='d', cmap='Blues')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xray_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
